{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efae5d-1ed6-4c91-82f9-82708646b17a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61efae5d-1ed6-4c91-82f9-82708646b17a",
    "outputId": "6f5d28a3-88bc-4a37-f2f7-a3d2ca280224"
   },
   "outputs": [],
   "source": [
    "!pip install torchvision\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c3d3f-8b1b-435c-b3df-8c7ffb1b0b7f",
   "metadata": {
    "id": "1d5c3d3f-8b1b-435c-b3df-8c7ffb1b0b7f"
   },
   "source": [
    "# Data Import & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa2552-5d0a-4b71-9087-add7c7fdded5",
   "metadata": {
    "id": "befa2552-5d0a-4b71-9087-add7c7fdded5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from os import path\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from typing import Sequence\n",
    "from torchvision.transforms import functional as F\n",
    "import numbers\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torchmetrics as TM\n",
    "\n",
    "# Convert a pytorch tensor into a PIL image\n",
    "t2img = T.ToPILImage()\n",
    "# Convert a PIL image into a pytorch tensor\n",
    "img2t = T.ToTensor()\n",
    "\n",
    "dataset_path = \"/Users/weijia/college/DS4400/FinalProject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5f7b7-cd9c-4e17-9df4-2a0588ebaaf9",
   "metadata": {
    "id": "e4f5f7b7-cd9c-4e17-9df4-2a0588ebaaf9"
   },
   "outputs": [],
   "source": [
    "# Oxford IIIT Pets Segmentation dataset loaded via torchvision.\n",
    "pets_train_orig = torchvision.datasets.OxfordIIITPet(root=dataset_path, split=\"trainval\", target_types=\"segmentation\", download=True)\n",
    "pets_test_orig = torchvision.datasets.OxfordIIITPet(root=dataset_path, split=\"test\", target_types=\"segmentation\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e4a03-a4b2-4ab8-82de-a3d4624be41e",
   "metadata": {
    "id": "020e4a03-a4b2-4ab8-82de-a3d4624be41e"
   },
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "class TrimapClasses(IntEnum):\n",
    "    PET = 0\n",
    "    BACKGROUND = 1\n",
    "    BORDER = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802ffe5-9ab2-43f8-bd23-1a16d40ed1b7",
   "metadata": {
    "id": "b802ffe5-9ab2-43f8-bd23-1a16d40ed1b7"
   },
   "outputs": [],
   "source": [
    "# Convert a float trimap ({1, 2, 3} / 255.0) into a float tensor with\n",
    "# pixel values in the range 0.0 to 1.0 so that the border pixels\n",
    "# can be properly displayed.\n",
    "def trimap2f(trimap):\n",
    "    return (img2t(trimap) * 255.0 - 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85249a37-4f0e-4307-925e-dc900ae66ae4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "85249a37-4f0e-4307-925e-dc900ae66ae4",
    "outputId": "4624b4c1-a7ba-4e67-f89b-a7d2b131e8fc"
   },
   "outputs": [],
   "source": [
    "idx_pet = 17 #@param {type:\"slider\", min:0, max:50, step:1}\n",
    "(train_pets_input, train_pets_target) = pets_train_orig[idx_pet]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(train_pets_input)\n",
    "plt.title(\"Image\"); plt.grid(False)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(t2img(trimap2f(train_pets_target)))\n",
    "plt.title(\"Label\"); plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ae101-dbbb-4425-8447-a6c7ef65e613",
   "metadata": {
    "id": "983ae101-dbbb-4425-8447-a6c7ef65e613"
   },
   "source": [
    "# Helper Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e7fff-a203-43f0-8ae1-d7f4f6271946",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "077e7fff-a203-43f0-8ae1-d7f4f6271946",
    "outputId": "d6a2767c-58ac-4afe-8945-c95c278b9023"
   },
   "outputs": [],
   "source": [
    "def save_model_checkpoint(model, cp_name):\n",
    "    torch.save(model.state_dict(), os.path.join(working_dir, cp_name))\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Load model from saved checkpoint\n",
    "def load_model_from_checkpoint(model, ckp_path):\n",
    "    return model.load_state_dict(\n",
    "        torch.load(\n",
    "            ckp_path,\n",
    "            map_location=get_device(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Send the Tensor or Model (input argument x) to the right device\n",
    "# for this notebook. i.e. if GPU is enabled, then send to GPU/CUDA\n",
    "# otherwise send to CPU.\n",
    "def to_device(x):\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cuda()\n",
    "    else:\n",
    "        return x.cpu()\n",
    "\n",
    "def get_model_parameters(m):\n",
    "    total_params = sum(\n",
    "        param.numel() for param in m.parameters()\n",
    "    )\n",
    "    return total_params\n",
    "\n",
    "def print_model_parameters(m):\n",
    "    num_model_parameters = get_model_parameters(m)\n",
    "    print(f\"The Model has {num_model_parameters} parameters\")\n",
    "# end if\n",
    "\n",
    "def close_figures():\n",
    "    while len(plt.get_fignums()) > 0:\n",
    "        plt.close()\n",
    "    # end while\n",
    "# end def\n",
    "\n",
    "# Validation: Check if CUDA is available\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6492f9-8173-410c-a3c9-6bc9ee4a1f42",
   "metadata": {
    "id": "ee6492f9-8173-410c-a3c9-6bc9ee4a1f42"
   },
   "outputs": [],
   "source": [
    "# Simple torchvision compatible transform to send an input tensor\n",
    "# to a pre-specified device.\n",
    "class ToDevice(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Sends the input object to the device specified in the\n",
    "    object's constructor by calling .to(device) on the object.\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, img):\n",
    "        return img.to(self.device)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(device={device})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fce43-6af0-45bb-a127-c2a5035753b9",
   "metadata": {
    "id": "8e9fce43-6af0-45bb-a127-c2a5035753b9"
   },
   "outputs": [],
   "source": [
    "# Create a dataset wrapper that allows us to perform custom image augmentations\n",
    "# on both the target and label (segmentation mask) images.\n",
    "#\n",
    "# These custom image augmentations are needed since we want to perform\n",
    "# transforms such as:\n",
    "# 1. Random horizontal flip\n",
    "# 2. Image resize\n",
    "#\n",
    "# and these operations need to be applied consistently to both the input\n",
    "# image as well as the segmentation mask.\n",
    "class OxfordIIITPetsAugmented(torchvision.datasets.OxfordIIITPet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        split: str,\n",
    "        target_types=\"segmentation\",\n",
    "        download=False,\n",
    "        pre_transform=None,\n",
    "        post_transform=None,\n",
    "        pre_target_transform=None,\n",
    "        post_target_transform=None,\n",
    "        common_transform=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            root=root,\n",
    "            split=split,\n",
    "            target_types=target_types,\n",
    "            download=download,\n",
    "            transform=pre_transform,\n",
    "            target_transform=pre_target_transform,\n",
    "        )\n",
    "        self.post_transform = post_transform\n",
    "        self.post_target_transform = post_target_transform\n",
    "        self.common_transform = common_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return super().__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (input, target) = super().__getitem__(idx)\n",
    "\n",
    "        # Common transforms are performed on both the input and the labels\n",
    "        # by creating a 4 channel image and running the transform on both.\n",
    "        # Then the segmentation mask (4th channel) is separated out.\n",
    "        if self.common_transform is not None:\n",
    "            both = torch.cat([input, target], dim=0)\n",
    "            both = self.common_transform(both)\n",
    "            (input, target) = torch.split(both, 3, dim=0)\n",
    "        # end if\n",
    "\n",
    "        if self.post_transform is not None:\n",
    "            input = self.post_transform(input)\n",
    "        if self.post_target_transform is not None:\n",
    "            target = self.post_target_transform(target)\n",
    "\n",
    "        return (input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0bac2-d0d9-4cbf-9cc8-a0ce5cf5677d",
   "metadata": {
    "id": "07a0bac2-d0d9-4cbf-9cc8-a0ce5cf5677d"
   },
   "source": [
    "# Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd084d-7c70-431e-83ca-fef7c82ca42e",
   "metadata": {
    "id": "53bd084d-7c70-431e-83ca-fef7c82ca42e"
   },
   "outputs": [],
   "source": [
    "# Train the model for a single epoch\n",
    "def train_model(model, loader, optimizer, cel=True, binary_classification=False):\n",
    "    to_device(model.train())\n",
    "    if cel:\n",
    "        criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    else:\n",
    "        criterion = IoULoss(softmax=True)\n",
    "    # end if\n",
    "    if binary_classification:\n",
    "        criterion = nn.BCELoss(reduction='mean')\n",
    "    # end if\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_samples = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(loader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = to_device(inputs)\n",
    "        targets = to_device(targets)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # The ground truth labels have a channel dimension (NCHW).\n",
    "        # We need to remove it before passing it into\n",
    "        # CrossEntropyLoss so that it has shape (NHW) and each element\n",
    "        # is a value representing the class of the pixel.\n",
    "        if cel:\n",
    "            targets = targets.squeeze(dim=1)\n",
    "        # end if\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_samples += targets.size(0)\n",
    "        running_loss += loss.item()\n",
    "    # end for\n",
    "\n",
    "    print(\"Trained {} samples, Loss: {:.4f}\".format(\n",
    "        running_samples,\n",
    "        running_loss / (batch_idx+1),\n",
    "    ))\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9610106-acec-4ae2-b337-b01bb3958f5d",
   "metadata": {
    "id": "a9610106-acec-4ae2-b337-b01bb3958f5d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define training loop. This will train the model for multiple epochs.\n",
    "#\n",
    "# epochs: A tuple containing the start epoch (inclusive) and end epoch (exclusive).\n",
    "#         The model is trained for [epoch[0] .. epoch[1]) epochs.\n",
    "#\n",
    "def train_loop(model, train_data_loader, test_data_loader, epochs, optimizer, scheduler, save_path, cel=True, binary_classification=False):\n",
    "    epoch_i, epoch_j = epochs\n",
    "    avg_iou = []\n",
    "    for i in range(epoch_i, epoch_j):\n",
    "        epoch = i\n",
    "        print(f\"Epoch: {i:02d}, Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        train_model(model, train_data_loader, optimizer, cel=cel, binary_classification=binary_classification)\n",
    "        # evaluation on 210 pictures\n",
    "        with torch.inference_mode():\n",
    "            iou_accuracy_sum = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(test_data_loader, 0):\n",
    "              to_device(model.eval())\n",
    "              predictions = model(to_device(inputs))\n",
    "              pred, pred_labels, pred_mask = get_prediction_mask(predictions)\n",
    "              iou_accuracy = IoUMetric(pred, targets)\n",
    "              iou_accuracy_sum += iou_accuracy\n",
    "              # only print out the first batch (21 pictures)\n",
    "              if batch_idx == 0:\n",
    "                # print_image_gt_pred(pred, pred_labels, pred_mask, inputs, targets, epoch=epoch, save_path=save_path, show_plot=(epoch == epoch_j-1), binary_classification=binary_classification)\n",
    "                  print_image_gt_pred(pred, pred_labels, pred_mask, inputs, targets, epoch=epoch, save_path=save_path, show_plot=True, binary_classification=binary_classification)\n",
    "              if batch_idx == 9: # only use the first 10 batches as validation set\n",
    "                    break\n",
    "            iou_accuracy_avg = iou_accuracy_sum / 10 # average accuracy for the 10 batches\n",
    "            avg_iou.append(iou_accuracy_avg)\n",
    "        # end with\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        # end if\n",
    "        print(\"\")\n",
    "    # end for\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    epoch_numbers = range(epoch_i, epoch_j)\n",
    "    plt.plot(epoch_numbers, avg_iou, linestyle='-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IoU (All Classes)')\n",
    "    plt.title('IoU vs. Training Epoch')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bacb431-9372-49df-9189-8cecd7f419df",
   "metadata": {
    "id": "0bacb431-9372-49df-9189-8cecd7f419df"
   },
   "source": [
    "Function for Visualization that will display the Target image, Ground Truth Labels, and Predicted Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a242193e-17a3-468b-89fd-c1e16099969b",
   "metadata": {
    "id": "a242193e-17a3-468b-89fd-c1e16099969b"
   },
   "outputs": [],
   "source": [
    "def print_test_dataset_masks(model, test_pets_targets, test_pets_labels, epoch, save_path, show_plot, binary_classification=False):\n",
    "    to_device(model.eval())\n",
    "    predictions = model(to_device(test_pets_targets))\n",
    "    test_pets_labels = to_device(test_pets_labels)\n",
    "    print_image_gt_pred(predictions, test_pets_targets, test_pets_labels, epoch, save_path, show_plot, binary_classification=binary_classification)\n",
    "\n",
    "def get_prediction_mask(predictions):\n",
    "    if predictions.ndim == 4: # there is a color channel dimension\n",
    "        pred = nn.Softmax(dim=1)(predictions) # the 3 channels(labels) for each pixel will sum to 1\n",
    "        pred_labels = pred.argmax(dim=1) # get the channel(labels) that has the highest possibility for each pixel\n",
    "    else:\n",
    "        pred_labels = predictions\n",
    "        pred_labels[predictions < 0.5] = 0\n",
    "        pred_labels[predictions >= 0.5] = 1\n",
    "        pred = pred_labels.unsqueeze(1)\n",
    "\n",
    "    # Add a value 1 dimension at dim=1\n",
    "    pred_labels = pred_labels.unsqueeze(1)\n",
    "    pred_mask = pred_labels.to(torch.float)\n",
    "    return pred, pred_labels, pred_mask\n",
    "\n",
    "# def print_image_gt_pred(predictions, test_pets_targets, test_pets_labels, epoch, save_path, show_plot, binary_classification=False):\n",
    "def print_image_gt_pred(pred, pred_labels, pred_mask, test_pets_targets, test_pets_labels, epoch, save_path, show_plot, binary_classification=False):\n",
    "    # pred_mask = get_prediction_mask(predictions)\n",
    "\n",
    "    if (not binary_classification):\n",
    "        iou = to_device(TM.classification.MulticlassJaccardIndex(3, average='micro', ignore_index=TrimapClasses.BACKGROUND))\n",
    "        pixel_metric = to_device(TM.classification.MulticlassAccuracy(3, average='micro'))\n",
    "    else:\n",
    "        iou = to_device(TM.classification.BinaryJaccardIndex(ignore_index=TrimapClasses.BACKGROUND))\n",
    "        pixel_metric = to_device(TM.classification.BinaryAccuracy())\n",
    "        \n",
    "    iou_accuracy_ignore_bg = iou(pred_mask, test_pets_labels)\n",
    "    pixel_accuracy = pixel_metric(pred_labels, test_pets_labels)\n",
    "    iou_all = IoUMetric(pred, test_pets_labels)\n",
    "\n",
    "    title = f'Epoch: {epoch:02d}, Accuracy[Pixel: {pixel_accuracy:.4f}, IoU (ignore background): {iou_accuracy_ignore_bg:.4f}, IoU (all classes): {iou_all:.4f}]'\n",
    "    print(title)\n",
    "\n",
    "    # Close all previously open figures.\n",
    "    close_figures()\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    fig.suptitle(title, fontsize=12)\n",
    "\n",
    "    fig.add_subplot(3, 1, 1)\n",
    "    plt.imshow(t2img(torchvision.utils.make_grid(test_pets_targets, nrow=7)))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Targets\")\n",
    "\n",
    "    fig.add_subplot(3, 1, 2)\n",
    "    if (binary_classification):\n",
    "        plt.imshow(t2img(torchvision.utils.make_grid(test_pets_labels.float(), nrow=7)))\n",
    "    else:\n",
    "        plt.imshow(t2img(torchvision.utils.make_grid(test_pets_labels.float() / 2.0, nrow=7)))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Ground Truth Labels\")\n",
    "\n",
    "    fig.add_subplot(3, 1, 3)\n",
    "    if (binary_classification):\n",
    "        plt.imshow(t2img(torchvision.utils.make_grid(pred_mask, nrow=7)))\n",
    "    else:\n",
    "        plt.imshow(t2img(torchvision.utils.make_grid(pred_mask / 2.0, nrow=7)))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Predicted Labels\")\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(os.path.join(save_path, f\"epoch_{epoch:02}.png\"), format=\"png\", bbox_inches=\"tight\", pad_inches=0.4)\n",
    "    # end if\n",
    "\n",
    "    if show_plot is False:\n",
    "        close_figures()\n",
    "    else:\n",
    "        plt.show()\n",
    "    # end if\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472622de-d2e1-4d86-86f9-84f39dcc8bd9",
   "metadata": {
    "id": "472622de-d2e1-4d86-86f9-84f39dcc8bd9"
   },
   "source": [
    "# Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b466a9-877e-4bd5-b608-36273652f7ae",
   "metadata": {
    "id": "b9b466a9-877e-4bd5-b608-36273652f7ae"
   },
   "outputs": [],
   "source": [
    "# Define a custom IoU Metric for validating the model.\n",
    "def IoUMetric(pred, gt, softmax=False):\n",
    "    # Run softmax if input is logits.\n",
    "    if softmax is True:\n",
    "        pred = nn.Softmax(dim=1)(pred)\n",
    "    # end if\n",
    "\n",
    "    # Add the one-hot encoded masks for all 3 output channels\n",
    "    # (for all the classes) to a tensor named 'gt' (ground truth).\n",
    "    gt = torch.cat([ (gt == i) for i in range(3) ], dim=1)\n",
    "\n",
    "    intersection = gt * pred\n",
    "    union = gt + pred - intersection\n",
    "\n",
    "    # Compute the sum over all the dimensions except for the batch dimension.\n",
    "    iou = (intersection.sum(dim=(1, 2, 3)) + 0.001) / (union.sum(dim=(1, 2, 3)) + 0.001)\n",
    "\n",
    "    # Compute the mean over the batch dimension.\n",
    "    return iou.mean()\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, softmax=False):\n",
    "        super().__init__()\n",
    "        self.softmax = softmax\n",
    "\n",
    "    # pred => Predictions (logits, B, 3, H, W)\n",
    "    # gt => Ground Truth Labales (B, 1, H, W)\n",
    "    def forward(self, pred, gt):\n",
    "        # return 1.0 - IoUMetric(pred, gt, self.softmax)\n",
    "        # Compute the negative log loss for stable training.\n",
    "        return -(IoUMetric(pred, gt, self.softmax).log())\n",
    "    # end def\n",
    "# end class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db63c9d0-ebbb-4b36-be42-e2f05fb6956e",
   "metadata": {
    "id": "db63c9d0-ebbb-4b36-be42-e2f05fb6956e"
   },
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CGoakaGBC2Wu",
   "metadata": {
    "id": "CGoakaGBC2Wu"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lrqIkbZ65nKh",
   "metadata": {
    "id": "lrqIkbZ65nKh"
   },
   "source": [
    "### Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9ooQrmwjy",
   "metadata": {
    "id": "d3a9ooQrmwjy"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "w    def forward(self, x):\n",
    "        original_shape = x.shape\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = x.reshape(batch_size, channels, -1).permute(0, 2, 1)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.reshape(batch_size, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UsioAn4G5P27",
   "metadata": {
    "id": "UsioAn4G5P27"
   },
   "source": [
    "### FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FY9Hw6rQ5QFp",
   "metadata": {
    "id": "FY9Hw6rQ5QFp"
   },
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.e11 = nn.Conv2d(3, 8, kernel_size=3, padding=1)\n",
    "        self.e12 = nn.Conv2d(8, 8, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) #original size / 2\n",
    "\n",
    "        self.e21 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.e22 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) #original size / 4\n",
    "\n",
    "        self.e31 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.e32 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) #original size / 8\n",
    "\n",
    "        self.e41 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.e42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) #original size / 16\n",
    "\n",
    "        self.e51 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.e52 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 32, kernel_size=16, stride=16)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "\n",
    "        score = self.bn1(self.relu(self.deconv1(xe52)))\n",
    "        score = self.classifier(score)\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_0ljpJeI2cs7",
   "metadata": {
    "id": "_0ljpJeI2cs7"
   },
   "source": [
    "### U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf32d42-3e38-490a-980f-aad8a8af3c08",
   "metadata": {
    "id": "2cf32d42-3e38-490a-980f-aad8a8af3c08"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image.\n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "\n",
    "        self.e11 = nn.Conv2d(3, 8, kernel_size=3, padding=1)\n",
    "        self.e12 = nn.Conv2d(8, 8, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e21 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.e22 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e31 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.e32 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e41 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.e42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e51 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.e52 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(128, 64, kernel_size=3, padding=1) # input channel = 128 because the input will be concat of result from upconv1 and encoder layer e42\n",
    "        self.d12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(8, 8, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(8, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ai9bhGIOto4U",
   "metadata": {
    "id": "Ai9bhGIOto4U"
   },
   "source": [
    "### DeepLabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CMAEpRsYtoFM",
   "metadata": {
    "id": "CMAEpRsYtoFM"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "class DeepLabV3Plus(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepLab v3+: Dilated ResNet with multi-grid + improved ASPP + decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes, n_blocks, atrous_rates, multi_grids, output_stride):\n",
    "        super(DeepLabV3Plus, self).__init__()\n",
    "\n",
    "        # Stride and dilation\n",
    "        if output_stride == 8:\n",
    "            s = [1, 2, 1, 1]\n",
    "            d = [1, 1, 2, 2]\n",
    "        elif output_stride == 16:\n",
    "            s = [1, 2, 2, 1]\n",
    "            d = [1, 1, 1, 2]\n",
    "\n",
    "        # Encoder\n",
    "        ch = [8 * 2 ** p for p in range(6)]\n",
    "        self.layer1 = _Stem(ch[0])\n",
    "        self.layer2 = _ResLayer(n_blocks[0], ch[0], ch[2], s[0], d[0])\n",
    "        self.layer3 = _ResLayer(n_blocks[1], ch[2], ch[3], s[1], d[1])\n",
    "        self.layer4 = _ResLayer(n_blocks[2], ch[3], ch[4], s[2], d[2])\n",
    "        self.layer5 = _ResLayer(n_blocks[3], ch[4], ch[5], s[3], d[3], multi_grids)\n",
    "        self.aspp = _ASPP(ch[5], ch[2], atrous_rates)\n",
    "        concat_ch = ch[2] * (len(atrous_rates) + 2)\n",
    "        self.add_module(\"fc1\", _ConvBnReLU(concat_ch, ch[2], 1, 1, 0, 1))\n",
    "\n",
    "        # Decoder\n",
    "        self.reduce = _ConvBnReLU(ch[2], ch[0], 1, 1, 0, 1)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"conv1\", _ConvBnReLU(ch[2] + ch[0], ch[2], 3, 1, 1, 1)),\n",
    "                    (\"conv2\", _ConvBnReLU(ch[2], ch[2], 3, 1, 1, 1)),\n",
    "                    (\"conv3\", nn.Conv2d(ch[2], n_classes, kernel_size=1)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.layer1(x)\n",
    "        h = self.layer2(h)\n",
    "        h_ = self.reduce(h)\n",
    "        h = self.layer3(h)\n",
    "        h = self.layer4(h)\n",
    "        h = self.layer5(h)\n",
    "        h = self.aspp(h)\n",
    "        h = self.fc1(h)\n",
    "        h = interpolate(h, size=h_.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        h = torch.cat((h, h_), dim=1)\n",
    "        h = self.fc2(h)\n",
    "        h = interpolate(h, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        return h\n",
    "\n",
    "\n",
    "class _ResLayer(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Residual layer with multi grids\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, in_ch, out_ch, stride, dilation, multi_grids=None):\n",
    "        super(_ResLayer, self).__init__()\n",
    "\n",
    "        if multi_grids is None:\n",
    "            multi_grids = [1 for _ in range(n_layers)]\n",
    "        else:\n",
    "            assert n_layers == len(multi_grids)\n",
    "\n",
    "        # Downsampling is only in the first block\n",
    "        for i in range(n_layers):\n",
    "            self.add_module(\n",
    "                \"block{}\".format(i + 1),\n",
    "                _Bottleneck(\n",
    "                    in_ch=(in_ch if i == 0 else out_ch),\n",
    "                    out_ch=out_ch,\n",
    "                    stride=(stride if i == 0 else 1),\n",
    "                    dilation=dilation * multi_grids[i],\n",
    "                    downsample=(True if i == 0 else False),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "\n",
    "class _Stem(nn.Sequential):\n",
    "    \"\"\"\n",
    "    The 1st conv layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_ch):\n",
    "        super(_Stem, self).__init__()\n",
    "        self.add_module(\"conv1\", _ConvBnReLU(3, out_ch, 7, 2, 3, 1)) # size /=2\n",
    "        self.add_module(\"pool\", nn.MaxPool2d(3, 2, 1, ceil_mode=True)) # size /=2\n",
    "\n",
    "\n",
    "class _ASPP(nn.Module):\n",
    "    \"\"\"\n",
    "    Atrous spatial pyramid pooling with image-level feature\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, rates):\n",
    "        super(_ASPP, self).__init__()\n",
    "        self.stages = nn.Module()\n",
    "        self.stages.add_module(\"c0\", _ConvBnReLU(in_ch, out_ch, 1, 1, 0, 1))\n",
    "        for i, rate in enumerate(rates):\n",
    "            self.stages.add_module(\n",
    "                \"c{}\".format(i + 1),\n",
    "                _ConvBnReLU(in_ch, out_ch, 3, 1, padding=rate, dilation=rate),\n",
    "            )\n",
    "        self.stages.add_module(\"imagepool\", _ImagePool(in_ch, out_ch))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([stage(x) for stage in self.stages.children()], dim=1)\n",
    "\n",
    "class _ImagePool(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = _ConvBnReLU(in_ch, out_ch, 1, 1, 0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        h = self.pool(x)\n",
    "        h = self.conv(h)\n",
    "        h = interpolate(h, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "        return h\n",
    "\n",
    "\n",
    "_BOTTLENECK_EXPANSION = 4\n",
    "\n",
    "class _Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Bottleneck block of MSRA ResNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, stride, dilation, downsample):\n",
    "        super(_Bottleneck, self).__init__()\n",
    "        mid_ch = out_ch // _BOTTLENECK_EXPANSION\n",
    "        self.reduce = _ConvBnReLU(in_ch, mid_ch, 1, stride, 0, 1, True)\n",
    "        self.conv3x3 = _ConvBnReLU(mid_ch, mid_ch, 3, 1, dilation, dilation, True)\n",
    "        self.increase = _ConvBnReLU(mid_ch, out_ch, 1, 1, 0, 1, False)\n",
    "        self.shortcut = (\n",
    "            _ConvBnReLU(in_ch, out_ch, 1, stride, 0, 1, False)\n",
    "            if downsample\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.reduce(x)\n",
    "        h = self.conv3x3(h)\n",
    "        h = self.increase(h)\n",
    "        h += self.shortcut(x)\n",
    "        return relu(h)\n",
    "\n",
    "class _ConvBnReLU(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Cascade of 2D convolution, batch norm, and ReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_ch, out_ch, kernel_size, stride, padding, dilation, relu=True\n",
    "    ):\n",
    "        super(_ConvBnReLU, self).__init__()\n",
    "        self.add_module(\n",
    "            \"conv\",\n",
    "            nn.Conv2d(\n",
    "                in_ch, out_ch, kernel_size, stride, padding, dilation, bias=False\n",
    "            ),\n",
    "        )\n",
    "        self.add_module(\"bn\", nn.BatchNorm2d(out_ch, eps=1e-5, momentum=1 - 0.999))\n",
    "\n",
    "        if relu:\n",
    "            self.add_module(\"relu\", nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342d79c-a2e5-49e0-a0d5-01dabea29b34",
   "metadata": {
    "id": "0342d79c-a2e5-49e0-a0d5-01dabea29b34"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb971bf6-00da-4e45-9f34-12320bc3a594",
   "metadata": {
    "id": "cb971bf6-00da-4e45-9f34-12320bc3a594"
   },
   "outputs": [],
   "source": [
    "# Create a tensor for a segmentation trimap.\n",
    "# Input: Float tensor with values in [0.0 .. 1.0]\n",
    "# Output: Long tensor with values in {0, 1, 2}\n",
    "def tensor_trimap(t):\n",
    "    x = t * 255\n",
    "    x = x.to(torch.long)\n",
    "    x = x - 1\n",
    "    return x\n",
    "\n",
    "def args_to_dict(**kwargs):\n",
    "    return kwargs\n",
    "\n",
    "transform_dict = args_to_dict(\n",
    "    pre_transform=T.ToTensor(),\n",
    "    pre_target_transform=T.ToTensor(),\n",
    "    common_transform=T.Compose([\n",
    "        ToDevice(get_device()),\n",
    "        T.Resize((128, 128), interpolation=T.InterpolationMode.NEAREST),\n",
    "        # Random Horizontal Flip as data augmentation.\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "    ]),\n",
    "    post_transform=T.Compose([\n",
    "        # Color Jitter as data augmentation.\n",
    "        T.ColorJitter(contrast=0.3),\n",
    "    ]),\n",
    "    post_target_transform=T.Compose([\n",
    "        T.Lambda(tensor_trimap),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "# Create the train and test instances of the data loader for the\n",
    "# Oxford IIIT Pets dataset with random augmentations applied.\n",
    "# The images are resized to 128x128 squares, so the aspect ratio\n",
    "# will be chaged. We use the nearest neighbour resizing algorithm\n",
    "# to avoid disturbing the pixel values in the provided segmentation\n",
    "# mask.\n",
    "pets_train = OxfordIIITPetsAugmented(\n",
    "    root=dataset_path,\n",
    "    split=\"trainval\",\n",
    "    target_types=\"segmentation\",\n",
    "    download=False,\n",
    "    **transform_dict,\n",
    ")\n",
    "pets_test = OxfordIIITPetsAugmented(\n",
    "    root=dataset_path,\n",
    "    split=\"test\",\n",
    "    target_types=\"segmentation\",\n",
    "    download=False,\n",
    "    **transform_dict,\n",
    ")\n",
    "\n",
    "pets_train_loader = torch.utils.data.DataLoader(\n",
    "    pets_train,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "pets_test_loader = torch.utils.data.DataLoader(\n",
    "    pets_test,\n",
    "    batch_size=21,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e-U5rcNSnX-Z",
   "metadata": {
    "id": "e-U5rcNSnX-Z"
   },
   "outputs": [],
   "source": [
    "# Create a tensor for a segmentation trimap.\n",
    "# Input: Float tensor with values in [0.0 .. 1.0]\n",
    "# Output: Float tensor with values in {0, 1} with all border classified as pet\n",
    "def binary_tensor_trimap(t):\n",
    "    x = t * 255\n",
    "    x = x - 1\n",
    "    x[x == TrimapClasses.BORDER] = TrimapClasses.PET\n",
    "    return x\n",
    "\n",
    "binary_transform_dict = args_to_dict(\n",
    "    pre_transform=T.ToTensor(),\n",
    "    pre_target_transform=T.ToTensor(),\n",
    "    common_transform=T.Compose([\n",
    "        ToDevice(get_device()),\n",
    "        T.Resize((128, 128), interpolation=T.InterpolationMode.NEAREST),\n",
    "        # Random Horizontal Flip as data augmentation.\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "    ]),\n",
    "    post_transform=T.Compose([\n",
    "        # Color Jitter as data augmentation.\n",
    "        T.ColorJitter(contrast=0.3),\n",
    "    ]),\n",
    "    post_target_transform=T.Compose([\n",
    "        T.Lambda(binary_tensor_trimap),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "binary_pets_train = OxfordIIITPetsAugmented(\n",
    "    root=dataset_path,\n",
    "    split=\"trainval\",\n",
    "    target_types=\"segmentation\",\n",
    "    download=False,\n",
    "    **binary_transform_dict,\n",
    ")\n",
    "binary_pets_test = OxfordIIITPetsAugmented(\n",
    "    root=dataset_path,\n",
    "    split=\"test\",\n",
    "    target_types=\"segmentation\",\n",
    "    download=False,\n",
    "    **binary_transform_dict,\n",
    ")\n",
    "\n",
    "binary_pets_train_loader = torch.utils.data.DataLoader(\n",
    "    binary_pets_train,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "binary_pets_test_loader = torch.utils.data.DataLoader(\n",
    "    binary_pets_test,\n",
    "    batch_size=21,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-CSClXqSssZh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-CSClXqSssZh",
    "outputId": "806f0a79-2261-4c43-d33d-f6898808e84f"
   },
   "outputs": [],
   "source": [
    "logistics_regression_model = LogisticRegression()\n",
    "optimizer = torch.optim.Adam(logistics_regression_model.parameters(), lr=0.0001)\n",
    "to_device(logistics_regression_model)\n",
    "train_loop(logistics_regression_model, binary_pets_train_loader, binary_pets_test_loader, (1, 11), optimizer, scheduler=None, save_path=None, binary_classification=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rGzBFzp2bEw0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rGzBFzp2bEw0",
    "outputId": "988f1b53-2cfe-4838-8fcc-4e19da00803b"
   },
   "outputs": [],
   "source": [
    "fcn_model = FCN(n_class=3)\n",
    "optimizer = torch.optim.Adam(fcn_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "to_device(fcn_model)\n",
    "train_loop(fcn_model, pets_train_loader, pets_test_loader, (1, 51), optimizer, scheduler, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283d27e-d981-45a4-962a-2610d691900c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d283d27e-d981-45a4-962a-2610d691900c",
    "outputId": "d7d6ce0c-2128-4226-9919-185c4562286d"
   },
   "outputs": [],
   "source": [
    "unet_model = UNet(n_class=3)\n",
    "optimizer = torch.optim.Adam(unet_model.parameters(), lr=0.002)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "to_device(unet_model)\n",
    "train_loop(unet_model, pets_train_loader, pets_test_loader, (1, 51), optimizer, scheduler, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hYFPpn0hSU83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYFPpn0hSU83",
    "outputId": "628e9b62-237f-43c2-900f-351c8a140b8a"
   },
   "outputs": [],
   "source": [
    "deeplab_model = DeepLabV3Plus(n_classes=3, n_blocks=[3, 4, 9, 3], atrous_rates=[1, 2, 3], multi_grids=[1, 2, 1], output_stride=8)\n",
    "optimizer = torch.optim.Adam(deeplab_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "to_device(deeplab_model)\n",
    "train_loop(deeplab_model, pets_train_loader, pets_test_loader, (1, 101), optimizer, scheduler, save_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XEUar5rkrYwF",
   "metadata": {
    "id": "XEUar5rkrYwF"
   },
   "source": [
    "# Testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IN3n8gp0pZNV",
   "metadata": {
    "id": "IN3n8gp0pZNV"
   },
   "outputs": [],
   "source": [
    "def test_dataset_accuracy(model, loader, binary_classification=False):\n",
    "    to_device(model.eval())\n",
    "\n",
    "    if (not binary_classification):\n",
    "        iou = to_device(TM.classification.MulticlassJaccardIndex(3, average='micro', ignore_index=TrimapClasses.BACKGROUND))\n",
    "        pixel_metric = to_device(TM.classification.MulticlassAccuracy(3, average='micro'))\n",
    "    else:\n",
    "        iou = to_device(TM.classification.BinaryJaccardIndex(ignore_index=TrimapClasses.BACKGROUND))\n",
    "        pixel_metric = to_device(TM.classification.BinaryAccuracy())\n",
    "\n",
    "    iou_accuracies = []\n",
    "    pixel_accuracies = []\n",
    "    custom_iou_accuracies = []\n",
    "\n",
    "    print_model_parameters(model)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(loader, 0):\n",
    "        if batch_idx < 10:\n",
    "            continue\n",
    "        inputs = to_device(inputs)\n",
    "        targets = to_device(targets)\n",
    "        predictions = model(inputs)\n",
    "\n",
    "        # pred_probabilities = nn.Softmax(dim=1)(predictions)\n",
    "        # pred_labels = predictions.argmax(dim=1)\n",
    "\n",
    "        # # Add a value 1 dimension at dim=1\n",
    "        # pred_labels = pred_labels.unsqueeze(1)\n",
    "        # # print(\"pred_labels.shape: {}\".format(pred_labels.shape))\n",
    "        # pred_mask = pred_labels.to(torch.float)\n",
    "\n",
    "        pred_probabilities, pred_labels, pred_mask = get_prediction_mask(predictions)\n",
    "\n",
    "\n",
    "        iou_accuracy = iou(pred_mask, targets)\n",
    "        iou_accuracies.append(iou_accuracy.item())\n",
    "\n",
    "        pixel_accuracy = pixel_metric(pred_labels, targets)\n",
    "        custom_iou = IoUMetric(pred_probabilities, targets)\n",
    "        pixel_accuracies.append(pixel_accuracy.item())\n",
    "        custom_iou_accuracies.append(custom_iou.item())\n",
    "\n",
    "        del inputs\n",
    "        del targets\n",
    "        del predictions\n",
    "    # end for\n",
    "\n",
    "    iou_tensor = torch.FloatTensor(iou_accuracies)\n",
    "    pixel_tensor = torch.FloatTensor(pixel_accuracies)\n",
    "    custom_iou_tensor = torch.FloatTensor(custom_iou_accuracies)\n",
    "\n",
    "    print(\"Test Dataset Accuracy :\")\n",
    "    print(f\"Pixel Accuracy: {pixel_tensor.mean():.4f}, IoU (ignore background): {iou_tensor.mean():.4f}, IoU (all classes): {custom_iou_tensor.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YzdbRxNppVnH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YzdbRxNppVnH",
    "outputId": "457aaf98-1e29-471c-fe15-66e6a78a22ec"
   },
   "outputs": [],
   "source": [
    "# logistics regression model\n",
    "with torch.inference_mode():\n",
    "    test_dataset_accuracy(logistics_regression_model, binary_pets_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kIon_boZzSjG",
   "metadata": {
    "id": "kIon_boZzSjG"
   },
   "outputs": [],
   "source": [
    "# FCN model\n",
    "with torch.inference_mode():\n",
    "    test_dataset_accuracy(fcn_model, pets_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2RkB3Z2nzaJk",
   "metadata": {
    "id": "2RkB3Z2nzaJk"
   },
   "outputs": [],
   "source": [
    "# U-net model\n",
    "with torch.inference_mode():\n",
    "    test_dataset_accuracy(unet_model, pets_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HMkb_XnHzdV5",
   "metadata": {
    "id": "HMkb_XnHzdV5"
   },
   "outputs": [],
   "source": [
    "# DeepLabV3+ model\n",
    "with torch.inference_mode():\n",
    "    test_dataset_accuracy(deeplab_model, pets_test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
